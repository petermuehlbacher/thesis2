The following section will follow \cite{TaoWSCL} quite closely while trying to be as self-contained as possible.

\section{Wigner's Semicircle Law}
\begin{definition}[Wigner matrix]
	A Wigner matrix $M$ is a complex, Hermitian matrix with independent and identically distributed entries $M_{ij}$ for $i\geq j$ and with mean $0$ and variance $1$ for $i>j$. The diagonal entries $M_{ii}$ have bounded mean and variance.
\end{definition}

% GOE special case

If $M_n$ is an $n$-dimensional Wigner matrix we know \todo{prove it}that the operator norm $\|M_n\|_{OP}$ is typically of size $\mathcal O(\sqrt n)$, so it natural to define the empirical spectral distribution (ESD) as follows:

\begin{definition}[ESD]
	\begin{equation*}
		\mu_{\frac{1}{\sqrt{n}}M_n}:=\frac{1}{n}\sum_{j=1}^n \delta_{\lambda_j(M_n)/\sqrt{n}},
	\end{equation*}
	where $\lambda_1(M_n)\leq\dots\leq\lambda_n(M_n)$ are the ordered, real eigenvalues of $M_n$.
\end{definition}

Since we are considering random matrices the ESDs will be random as well and thus it is interesting to ask if there is a measure on the real line $\mu$ such that it is the weak limit $\mu_{M_n/\sqrt{n}}\rightharpoonup\mu$ of $\mu_{M_n/\sqrt{n}}$, that is $\int_\mathbb{R} \varphi d\mu_{M_n/\sqrt{n}}$ converges in almost surely against $\int_\mathbb{R} \varphi d\mu$ for all $\varphi\in C_c(\mathbb R)$. This can also be derived from the more general definition of convergence in probability or almost surely, but we will not do that here.

Surprisingly such a limit $\mu$ exists and is even deterministic.

\begin{theorem}[Wigner's semicircle law]\label{thm:semicircle}
	Let $M_n$ be the top left $n\times n$ minors of an infinite Wigner matrix, then the ESDs $\mu_{M_n/\sqrt{n}}$ converge almost surely (and thus in probability) to the Wigner semicircle distribution given by
	\begin{equation*}
		\mu_{sc}:=\begin{cases}
						\frac{1}{2\pi}\sqrt{4-|x|^2}dx, &\mbox{if } |x|\leq 2 \\
						0, &\mbox{else}
					  \end{cases}=\frac{1}{2\pi}\sqrt{4-x^2}_+\diff x.
	\end{equation*}
\end{theorem}

A rough outline of the proof is given by this list of intermediary results that will be shown:
\begin{enumerate}
	\item Show that without loss of generality we can set the diagonal elements to zero and bound all other entries by some constant. Additionally some classic concentration of measure results will be shown.
	\item To show that $\mu_n\rightharpoonup\mu$ almost surely, it suffices to show that the respective Stieltjes transforms converge almost surely, pointwise in the upper half plane, i.e. $\mu_n\rightharpoonup\mu\Leftrightarrow\forall z\in\mathbb C:Im(z)>0 s_{\mu_n}(z)\rightarrow s_\mu(z)$ almost surely.
	\item The Stieltjes transform $s_n := s_{\mu_{M_n/\sqrt{n}}}$ is ``stable in $n$'', i.e. $s_n(z)=s_{n-1}(z)+\mathcal O(\frac{1}{n})$, where $\mathcal O$ can depend on $z$ and even $s_n(z)-\mathbb E s_n(z)\rightarrow 0$ almost surely.
	\item Derive the semicircle law by deriving the recursion $\mathbb E s_n(z)=-\frac{1}{z+\mathbb E s_n(z)}+o(1)$, where, again, $o(1)$ will depend on $z$ and ``inverting'' the Stieltjes transform.
\end{enumerate}
\begin{remark}
	Note that instead of step 4 one could have plugged in the semicircle distribution and simplified the proof by just checking that this is indeed the limit. This is not done here because we want to see how the Stieltjes transform method can be used to derive such a conclusion without knowing about it beforehand.
	
	Also, there are other proofs (e.g. \cite{scGOE}) specifically for the GOE/GUE (instead of the more general Wigner matrices) which exploit their symmetries to shorten the proof considerably.
\end{remark}

\subsection{Preliminary Reductions}\label{ssec:prelimreduct}
\begin{lemma}
	For the matrices $M_n$ as given in \ref{thm:semicircle} it can be assumed without loss of generality that the diagonal entries are zero and the absolute values $|[M_n]_{ij}|$ are bounded by some constant $C>0$ which does not depend on $i,j$ or $n$.
\end{lemma}
\begin{proof}
%	For every $n$ define

%$$\overline[X_n]_{ij}:=\begin{cases}
%						[M_n]_{ij}/\sqrt n\mathbf 1_{|M_n|\leq C}-\mathbb E\left[[M_n]_{ij}\mathbf 1_{|M_n|\leq C}\right], &\mbox{if } i\neq j \\
%						0, &\mbox{if } i=j
%					  \end{cases}.$$
					  
\todo{decide whether to take more general approach (T.Tao) or the straightforward one (Feier)}
\end{proof}

\begin{theorem}[McDiarmid's inequality]
	Let $X_1,\dots,X_n$ be independent random variables taking values in ranges $R_1,\dots,R_n$ and let $F:R_1\times\dots\times R_n\rightarrow\mathbb C$, such that for every $1\leq i\leq n$ we have $|F(x_1,\dots,x_i,\dots,x_n)-F(x_1,\dots,x_i',\dots,x_n)|\leq c_i$. Then for any $\lambda>0$ one has $$\mathbb P(|F(X)-\mathbb EF(X)|>\lambda\sigma)\leq C\exp^{-c\lambda^2},$$ for some absolute\footnote{Constants that maintain the same value wherever they occur. In particular applying McDiarmid's inequality in different settings we do not need to consider $C_n, c_n$, but can still write $C,c$.} constants $c,C>0$ and $\sigma=\sum_{i=1}^n c_i^2$.
\end{theorem}
\todo{prove it}

\begin{lemma}[Borel-Cantelli]
	Given some random variables $(X_n)_{n=1}^\infty,X$ such that $\sum_n \mathbb P(d(X_n,X)\geq\varepsilon)<\infty$ for every $\varepsilon>0$, then $X_n$ converges almost surely to $X$ (in the topology induced by $d$).
\end{lemma}
\todo{prove it}

\begin{theorem}[Talagrand's concentration inequality]
	Let $K>0$ and $X_1,\dots,X_n$ be independent complex variables with $|X_i|<K$ for all $1\leq i\leq n$. Identifying $\mathbb C$ with $\mathbb R^2$, let $F:\mathbb C^n\rightarrow\mathbb R$ a $1$-Lipschitz, convex function. Then for every $\lambda>0$ one has
	$$\mathbb P(|F(X)-MF(X)|\geq \lambda K)\leq C\exp^{-c\lambda^2}$$ and $$\mathbb P(|F(X)-\mathbb EF(X)|\geq \lambda K)\leq C\exp^{-c\lambda^2}$$
	for some absolute constants $c,C>0$, where $MF(X)$ is the median of $F(X)$.
\end{theorem}
\todo{prove it}

\begin{theorem}[Wielandt-Hoffmann inequality]
	For Hermitian\footnote{The original Wielandt-Hoffmann inequality holds for normal operators, but we will restrict ourselves to Hermitian ones for simplicity's sake here.} $A,B$, where $\|B\|_F^2:=tr(B^2)^{\frac{1}{2}}$ is the Frobenius norm, we have
	$$\sum_{j=1}^n|\lambda_j(A+B)-\lambda_j(A)|^2\leq\|B\|_F^2.$$
\end{theorem}
\todo{prove it}

\subsection{Stieltjes Transform}

\begin{definition}[Stieltjes transform]
	For a probability measure $\mu$ we write $s_\mu$ for its Stieltjes transform $$\int_\mathbb{R} \frac{1}{x-z}d\mu(x).$$
\end{definition}
As mentioned above, $s_n$ will be a shorthand for $s_{\mu_{M_n/\sqrt{n}}}$.

\begin{lemma}[Properties of the Stieltjes transform]\label{lm:stieltjesproperties}
In the following let $\mu$ be some probability measure.
	\begin{enumerate}
		\item For $z=a+ib$ we have $Im\frac{1}{x-z}=\frac{b}{(x-a)^2+b^2}>0$.\label{lm:stieltjesproperties1}
		\item $s_\mu$ is analytic in $\mathbb C\setminus supp(\mu)\supset\mathbb C_+$.\label{lm:stieltjesproperties2}
		\item We can bound the absolute value as well as the derivatives by $|\frac{d^j}{dz^j}s_\mu(z)|\leq \mathcal O(|Im(z)|^{-(j+1)})$ for all $j\in\mathbb \{0,1,\dots\}$.\label{lm:stieltjesproperties3}
	\end{enumerate}
\end{lemma}

\begin{proof}
	The first property is trivial, the second one can be seen by integrating $s_\mu$ over any contour not containing the support of $\mu$, interchanging the order of integration and noting that integrating $\frac{1}{x-z}$ gives $0$ by Cauchy's integral formula ($\frac{1}{x-z}$ being holomorphic outside the support of $\mu$). The Stieltjes transform being holomorphic (and thus analytic) follows by Morera's theorem.
	
	The third property can be obtained by using $\frac{1}{x-z}\leq \frac{1}{Im(z)}$ and using Cauchy's integral formula integrating this inequality.
\end{proof}

\begin{corollary}
	From \ref{lm:stieltjesproperties}.\ref{lm:stieltjesproperties1} it follows that $s_\mu$ is a Herglotz function and thus (e.g. \cite{TeschlQM}) $Im(s_\mu(.+ib))\rightharpoonup\pi\mu$ as $b\rightarrow 0^+$ in the vague topology or equivalently (by $\overline{s_\mu(z)}=s_\mu(\overline z)$)
	\begin{equation}
		\frac{s_\mu(.+ib)-s_\mu(.-ib)}{2\pi i}\rightharpoonup\mu.
	\end{equation}
	Note that this can also be seen by writing $Im(s_\mu)$ as the convolution $\pi\mu * P_b(a)$ with the Poisson kernels $P_b(x):=\frac{1}{\pi}\frac{b}{x^2+b^2} = \frac{1}{b}P_1(\frac{x}{b})$ which form a family of approximations to the identity.
	%This ``intuition'' will become important for the next proof and when applying Cauchy's interlacing law to show that $s_n(z)=s_{n-1}(z)+\mathcal O(\frac{1}{n})$.
\end{corollary}

\begin{theorem}[Stieltjes continuity theorem]
	For $\mu_n$ random measures and $\mu$ a deterministic measure the following statement holds:
	
	$\mu_n\rightharpoonup\mu$ almost surely in the vague topology if and only if $s_{\mu_n}(z)\rightarrow s_\mu(z)$ almost surely for every $z\in\mathbb C_+$.
\end{theorem}

\begin{proof}
	$$\mathbb P(\{\limsup_{n\rightarrow\infty}d_v(\mu_n,\mu)=0\})=1$$
	$$\mathbb P(\{\forall \phi\in C_c(\mathbb R): \lim_{n\rightarrow\infty}\int_\mathbb{R}\phi \diff\mu_n=\int_\mathbb{R}\phi \diff\mu\})=1$$
	$$\forall z\in\mathbb C_+:\mathbb P(\{\lim_{n\rightarrow\infty} s_{\mu_n}(z)=s_\mu(z)\})=1$$

	``$\Rightarrow$'': If $\mu_n\rightharpoonup\mu$ in the vague topology almost surely against a deterministic limit $\mu$, then $\forall \phi\in C_c(\mathbb R): \lim_{n\rightarrow\infty}\int_\mathbb{R}\phi \diff\mu_n=\int_\mathbb{R}\phi \diff\mu$ by definition and, by taking the completion\todo{not sure}, for all bounded, continuous functions vanishing at infinity. The function $x\mapsto \frac{1}{x-z}$ for some $z\in\mathbb C$ with $Im(z)>0$ is bounded and continuous on $\mathbb R$ and hence $s_{\mu_n}(z)\rightarrow s_\mu(z)$ almost surely.
	
	``$\Leftarrow$'': One can, up to an arbitrary small error $\varepsilon>0$, approximate $\int_\mathbb{R}\phi\diff\mu$ by $\int_\mathbb{R}\phi*P_b \diff\mu = \frac{1}{\pi}\int_\mathbb{R}\phi(a)s_\mu(a+ib)\diff a$ (and analogously for $\mu_n$).
	Thus we have $\frac{1}{\pi}\int_\mathbb{R}\phi(a)(s_\mu(a+ib)-s_{\mu_n}(a+ib))\diff a$ being equal to the difference (we are interested in) $\int_\mathbb{R}\phi\diff\mu - \int_\mathbb{R}\phi\diff\mu_n$ up to an error $\varepsilon$.
	
	In order not to ``lose'' the almost sure convergence by integration\todo{not sure if that's the reason} (a summation over uncountable many summands) we approximate it by a Riemann sum (which is possible since we can choose the support $I$ of the test function $\phi$ as the boundaries of integration). The error for the middle sum is proportional to $\max_{x\in I}(|f''(x)|)|I|^3 n^{-2}$, which (for $f$ being the integrand), can be made arbitrarily small. (To be able to control the $\max_{x\in I}(|f''(x)|)$ one may have to approximate the continuous test functions $\phi$ by smooth (or at least twice differentiable) ones like in every partial differential course.)
	
	This discretized sum now goes to zero almost surely.
\end{proof}

\subsection{Stableness and Concentration of Measure}
In the following we keep using the notation as defined in \ref{thm:semicircle}. To show that $s_n(z)=s_{n-1}(z)+\mathcal O_z(1/n)$ we first need to prove the following theorem:

\begin{theorem}[Cauchy's interlacing theorem]
	For any $n\times n$ Hermitian matrix $A_n$ with top left minor $A_{n-1}$ and eigenvalues of descending order ($\lambda_i\geq\lambda_{i+1}$) we have:
	\begin{equation*}
		\lambda_{i+1}(A_n)\leq\lambda_i(A_{n-1})\leq\lambda_i(A_n), 
	\end{equation*}
	for all $1\leq i < n$.
\end{theorem}
\begin{proof}
Using the min-max/max-min theorems ($\lambda_i(A)=\inf_{dim(V)=n-i+1}\sup_{v\in V : \|v\|=1}\langle Av,v\rangle$ and $\lambda_i(A)=\sup_{dim(V)=i}\inf_{v\in V : \|v\|=1}\langle Av,v\rangle$ respectively, c.f. \cite{TeschlQM} p.141) and writing $S_{n-i+1}$ for $\{v\in span\{a_i,\dots,a_n\}: \|v\|=1\}$, where $A_{n-1}a_j=\lambda_j a_j$ and $P$ an orthogonal projection such that $P^*A_nP=A_{n-1}$ we have
	\begin{equation*}
		\lambda_i(A_{n-1}) =
		\sup_{v\in S_i,\|v\|=1}v^*A_{n-1}v =
		\sup_{v\in S_i,\|v\|=1}v^*P^*A_nPv \geq
		\inf_{dim(V)=n-i}\sup_{v\in V,\|v\|=1}v^*A_nv =
		\lambda_{i+1}(A_n),
	\end{equation*}
	and
	\begin{equation*}
		\lambda_i(A_n) =
		\inf_{dim(V)=n-i+1}\sup_{v\in V,\|v\|=1}v^*A_nv \geq
		\sup_{v\in S_i,\|v\|=1}v^*P^*A_nPv =
		\sup_{v\in S_i,\|v\|=1}v^*A_{n-1}v =
		\lambda_i(A_{n-1}).%,
	\end{equation*}
	%where $S_i:=span\{a_i,\dots,a_{n-1}\}$. Note that we implicitly used the isometric embedding $\mathbb C^{n-1}\rightarrow\mathbb C^n$ given by $(x_1,\dots,x_{n-1})\mapsto(x_1,\dots,x_{n-1},0)$ to make sense of the equation $\langle \underbrace{P^*A_nP}_{n-1\times n-1}v,v\rangle = \langle \underbrace{A_n}_{n\times n}Pv,Pv\rangle$.
\end{proof}

Remembering the identity $Im(s_{\mu_n(a+ib)}=\pi\mu*P_b(a)$ and that $supp\mu_n$ consists of finitely many points, we have $Im(s_{\mu_n})=\pi\frac{1}{n}\sum_{\lambda_i}\frac{b}{(\lambda_i-a)^2+b^2}$ which suggests that it is important to take a closer look at the function $x\mapsto\frac{b}{(x-a)^2+b^2}$ to compare $s_{\mu_n}$ with $s_{\mu_{n-1}}$.

\begin{lemma}
	For fixed $z\in\mathbb C_+$ the Stieltjes transform is ``stable'' in $n$, i.e.
	\begin{equation*}
		s_n(z)=s_{n-1}(z)+\mathcal O\left(\frac{1}{n}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	The idea is to use the Cauchy interlacing law and apply it to the previously mentioned identity by seeing that
	
	$$\sum_{j=1}^{n-1}\frac{b}{\lambda_j(M_{n-1})/\sqrt{n}-a}-\sum_{j=1}^n\frac{b}{\lambda_j(M_n)/\sqrt{n}-a}$$
	
	Up to the dimensional factors these two sums correspond to $s_n$ and $s_{n-1}$ and because of Cauchy's interlacing law this is an alternating sum, giving
	
	$$\sqrt{n(n-1)}s_{n-1}(\sqrt{n/(n-1)}(a+ib))-ns_n(a+ib)=\mathcal O\left(\frac{1}{n}\right).$$
	
	Now using the fact that the Stieltjes transform $s_n$ is analytic away from the support of $\mu_n$ (\ref{lm:stieltjesproperties}.\ref{lm:stieltjesproperties2}) and using the bound for its derivatives (\ref{lm:stieltjesproperties}.\ref{lm:stieltjesproperties3}) we can approximate $s_{n-1}(.)$ by $s_{n-1}(\sqrt{n/(n-1) . )}$ and hence the statement holds.
\end{proof}

Using McDiarmid's inequality one gets 

\begin{equation}\label{eq:concentrationOfStieltjesTransform}
	\mathbb P(|s_n(z)-\mathbb Es_n(z)|\geq\lambda/\sqrt n)\leq C\exp^{-c\lambda^2},
\end{equation}
 for all $\lambda>0$ and some constants $c,C>0$.

From the Borel-Cantelli lemma we see that for every $z$ away from the real line $s_n(z)-\mathbb Es_n(z)$ converges almost surely to zero since, for some fixed $\varepsilon>0$, the sum $\sum_n \mathbb P(d(s_n-\mathbb Es_n,0)\geq\varepsilon) \leq C\sum_n\exp^{-cn\varepsilon^2}<\infty$ which is obtained by setting $\lambda=\varepsilon\sqrt n$.

\subsection{Finding the Semicircle Law}

We start off by using the following identity $$s_n(z) := \int_\mathbb{R}\frac{1}{x-z}d\mu_n(x) = \frac{1}{n}tr\left(\frac{1}{\sqrt n}M_n-zI_n\right)^{-1},$$ which holds for every $z\in\mathbb C\setminus supp(\mu_n)$. Because of the linearity of the trace we also have

\begin{equation}\label{eq:schursComplementForEs}
	\mathbb Es_n(z) = \frac{1}{n}\sum_{j=1}^n\mathbb E\left[\left(\frac{1}{\sqrt n}M_n-zI_n\right)^{-1}\right]_{jj}=\mathbb E\left[\left(\frac{1}{\sqrt n}M_n-zI_n\right)^{-1}\right]_{nn},
\end{equation}

where the last equality holds because all of the random variables $\left[(M_n/\sqrt n-zI_n)^{-1}\right]_{jj}$ have the same distribution.

To calculate one entry of an inverse of a matrix we use Schur's complement, which tells us that (under the assumptions that all the occurring inverse matrices exist) $$[(M_n/\sqrt n-zI_n)^{-1}]_{nn}=-\left(z+\frac{1}{n}X^*(\frac{1}{\sqrt n}M_{n-1}-zI_{n-1})^{-1}X\right)^{-1},$$ where $X\in\mathbb C^{n-1}$ is the top right column of $M_n$ with the bottom entry removed and the diagonal elements have been set to zero as justified in \ref{ssec:prelimreduct}.

This inverse exists because, for $z\in\mathbb C_+$, the imaginary part $Q:=Im\left((\frac{1}{\sqrt n}M_{n-1}-zI_{n-1})^{-1}\right)$ is positive definite according to the spectral theorem. To see this notice that this holds for arbitrary Hermitian matrices $M$ (instead of $\frac{1}{\sqrt n}M_{n-1}$) since their spectrum is on the real line. Thus, by the spectral theorem, we can write $Q=Im\int\frac{1}{\mu_M-z}dM(\mu_M)$ for some projection valued measure $dM$ and since $x\mapsto\frac{1}{x-z}$ is a Herglotz function its imaginary part will be greater than zero for $z\in\mathbb C_+$. As a result the imaginary part of the integrand (which is the imaginary part of the eigenvalues) will be greater than zero.
We conclude by noticing that $Im(z)>0$ plus something of the form $\langle Qx,x\rangle$ for $Q\geq 0$ will have imaginary part strictly greater than zero and hence the inverse exists.

The next step is to get a better understanding of the resolvent $R:=(\frac{1}{\sqrt n}M_{n-1}-zI_{n-1})^{-1}$ and its product $\langle RX,X\rangle$. Clearly $R$ and $X$ are independent, so we may treat $R$ almost like a deterministic matrix. Furthermore, again due to the spectral theorem, $\|R\|_{op}$ is at most $\mathcal O(1)$. By the strong law of large numbers $\|X\|=O(\sqrt n)$ almost surely. \todo{in Tao's notes it says with ``overwhelming'' probability - why does he use Chernoff/Hoeffding instead of the strong law of large numbers?}

In the following we will show some results for some deterministic matrix $A$ which has roughly the same properties as $R$ (i.e. $A\geq 0$ and $\|A\|_{OP}=O(1)$).

Noting that the function $X\mapsto \sqrt{\langle AX,X\rangle}$ is a Lipschitz function with operator norm $O(1)$ and remembering from \ref{ssec:prelimreduct} that we can safely assume the entries to be bounded, we can invoke Talagrand's concentration inequality to get 

$$\mathbb P(|\sqrt{\langle AX,X\rangle}-M\sqrt{\langle AX,X\rangle}|\geq\lambda)\leq C\exp^{-c\lambda^2}$$

for any $\lambda>0$. On the other hand we have $\sqrt{\langle AX,X\rangle}=O(\|X\|)=O(\sqrt n)$ almost surely.\todo{same as above footnote, since it's using this result} Hence the median $M\sqrt{\langle AX,X\rangle}=O(\sqrt n)$ and considering the square $\langle AX,X\rangle$\todo{which amounts to multiplying $O(\sqrt n)$, so that's why it appears on the other side <-- how to make this formal?}, we conclude that

$$\mathbb P(|\langle AX,X\rangle-M\langle AX,X\rangle|\geq\lambda\sqrt n)\leq C\exp^{-c\lambda^2}$$

with some (possibly different) $c,C>0$.

We may\todo{why?} replace the median with the expected value, yielding

\begin{equation}\label{eq:concentrationOfXRXAroundTrR}
	\mathbb P(|\langle AX,X\rangle-\mathbb E\langle AX,X\rangle|\geq\lambda\sqrt n)\leq C\exp^{-c\lambda^2}
\end{equation}

for the the case where $A$ is deterministic and positive definite. One can extend this result to arbitrary matrices of operator norm $O(1)$ by noting that it holds for Hermitian matrices $M=M_+-M_-$ ($M_+\geq 0,M_-\geq 0,\|M\|_{OP}=O(1)$) by applying the triangle inequality and \dots\todo{how to get from Hermitian to arbitrary matrices? Would splitting up in Re and Im make sense?}

\begin{remark}\label{rem:conditionalExpectationForDeterministicResult}
	By using conditional expectations the above results also hold true for random matrices $R$ with $R\geq 0$ and $\|R\|_{OP}=O(1)$ as long as it is independent of $X$. The idea is to write all the above statements as $\mathbb P(E|\{A=R\})=\frac{\mathbb P(E\cap\{A=R\})}{\mathbb P(\{A=R\})}=\mathbb P(E)$ for all events $E$.
\end{remark}

Now we want to know what $\mathbb E\langle RX,X\rangle$ actually is. Because of the linearity of the expectation we write it as $\sum_{i,j=1}^{n-1}\mathbb E[\overline{X_i}R_{ij}X_i]$. Since the $X_i$ and $R_{ij}$ are independent we can write that as $\sum_{i,j=1}^{n-1}\mathbb E[\overline{X_i}X_i]\mathbb E[R_{ij}]$, but as the $X_i$ are iid with mean zero and variance one this double sum simplifies to the expectation of the trace of $R$ $$\sum_{i=1}^{n-1}\mathbb ER_{ii}.$$

Noticing that, up to some ``almost correct'' normalization factors, $tr(R)=tr\left((M_{n-1}/\sqrt n-zI_{n-1})^{-1}\right)$ is the Stieltjes transform $s_{n-1}(z)$. To be more precise we have $$tr(R)=n\sqrt{\frac{n}{n-1}}s_{n-1}\left(\sqrt{\frac{n}{n-1}}s_{n-1}z\right),$$
but because of the smoothness of the Stieltjes transform for $z\in\mathbb C_+$ these factors do not play a role in the limit $n\rightarrow\infty$, i.e. $tr(R)=n(s_{n-1}(z)+o(1))$.

So using the concentration of measure results for the Stieltjes transform (\ref{eq:concentrationOfStieltjesTransform}) and for $\langle AX,X\rangle$(\ref{eq:concentrationOfXRXAroundTrR}), remembering that latter also holds for random matrices as long as they are independent (\ref{rem:conditionalExpectationForDeterministicResult}), we see that

$$\langle RX,X\rangle=n(s_{n-1}(z)+o(1))$$

with overwhelming probability. Substituting back in Schur's complement (\ref{eq:schursComplementForEs}) we get $$\mathbb Es_n(z)=-(z+\mathbb Es_n(z))^{-1}+o(1).$$

To say something about the limit we first need to ensure $\lim_{n\rightarrow\infty}\mathbb Es_n$ exists. This is indeed the case since $\mathbb Es_n$ is locally equicontinuous and locally uniformly bounded away from the real line. Applying the Arzel\'{a}-Ascoli theorem we get the existence of a subsequence that converges locally uniformly to a limit $s$, which is again a Herglotz function. Note that, by the concentration of measure for Stieltjes transforms, there is only one possible limit (so $\lim_{n\rightarrow\infty}\mathbb Es_n$ is well defined) and $s_n(z)$ even converges almost surely to $s(z)$. As a further result we get

$$s(z)=(z+s(z))^{-1},$$

where the quadratic formula gives $$s(z)=-\frac{z\pm\sqrt{z^2-4}}{2}$$

From $\lim_{a\rightarrow\infty}s_\mu(a+ib)=0$ for every Stieltjes transform of a fixed measure $\mu$ we see that we need to take $s(z)=\frac{-z+\sqrt{z^2-4}}{2}$.

We conclude the proof with the Stieltjes inversion formula, yielding the famous result

$$\frac{s(.+ib)-s(.-ib)}{2\pi i}\rightharpoonup\frac{1}{2\pi}\sqrt{4-x^2}_+\diff x=\mu_{sc}$$

as $b\rightarrow 0^+$, which can be verified by an application of the Cauchy integral formula.
























