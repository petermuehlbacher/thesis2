The following section will follow \cite{TaoWSCL} quite closely while trying to be as self-contained as possible.

\section{Wigner's Semicircle Law}
\begin{definition}[Wigner matrix]
	A Wigner matrix $M$ is a complex, Hermitian matrix with independent and identically distributed entries $M_{ij}$ for $i\geq j$ and with mean $0$ and variance $1$ for $i>j$. The diagonal entries $M_{ii}$ have bounded mean and variance.
\end{definition}

If $M_n$ is an $n$-dimensional Wigner matrix we know that the operator norm $\|M_n\|_{OP}$ is typically of size $O(\sqrt n)$, so it is natural to define the empirical spectral distribution (ESD) as follows:

\begin{definition}[ESD]
	\begin{equation*}
		\mu_{\frac{1}{\sqrt{n}}M_n}:=\frac{1}{n}\sum_{j=1}^n \delta_{\lambda_j(M_n)/\sqrt{n}},
	\end{equation*}
	where $\lambda_1(M_n)\leq\dots\leq\lambda_n(M_n)$ are the ordered, real eigenvalues of $M_n$.
\end{definition}

Since we are considering random matrices the ESDs will be random as well and thus it is interesting to ask if there is a measure on the real line $\mu$ such that it is the weak limit $\mu_{M_n/\sqrt{n}}\rightharpoonup\mu$ of $\mu_{M_n/\sqrt{n}}$, that is $\int_\mathbb{R} \varphi d\mu_{M_n/\sqrt{n}}$ converges in almost surely against $\int_\mathbb{R} \varphi d\mu$ for all $\varphi\in C_c(\mathbb R)$. This can also be derived from the more general definition of convergence in probability or almost surely, but we will not do that here.

Surprisingly such a limit $\mu$ exists and is even deterministic.

\begin{theorem}[Wigner's semicircle law]\label{thm:semicircle}
	Let $M_n$ be the top left $n\times n$ minors of an infinite Wigner matrix, then the ESDs $\mu_{M_n/\sqrt{n}}$ converge in probability\footnote{Even almost sure convergence can be shown, but we will not do so in this proof.} to the Wigner semicircle distribution given by
	\begin{equation*}
		\mu_{sc}:=\begin{cases}
						\frac{1}{2\pi}\sqrt{4-|x|^2}dx, &\mbox{if } |x|\leq 2 \\
						0, &\mbox{else}
					  \end{cases}=:\frac{1}{2\pi}\sqrt{4-x^2}_+\diff x.
	\end{equation*}
\end{theorem}

A rough outline of the proof is given by this list of intermediary results that will be shown:
\begin{enumerate}
	\item Show that without loss of generality we can set the diagonal elements to zero and bound all other entries by some constant. Additionally some classic concentration of measure results will be shown.
	\item To show that $\mu_n\rightharpoonup\mu$ almost surely, it suffices to show that the respective Stieltjes transforms converge almost surely, pointwise in the upper half plane, i.e. $\mu_n\rightharpoonup\mu\Leftrightarrow\forall z\in\mathbb C:Im(z)>0 s_{\mu_n}(z)\rightarrow s_\mu(z)$ almost surely.
	\item The Stieltjes transform $s_n := s_{\mu_{M_n/\sqrt{n}}}$ is ``stable in $n$'', i.e. $s_n(z)=s_{n-1}(z)+O(\frac{1}{n})$, where $O$ can depend on $z$ and even $s_n(z)-\mathbb E s_n(z)\rightarrow 0$ almost surely.
	\item Derive the semicircle law by deriving the recursion $\mathbb E s_n(z)=-\frac{1}{z+\mathbb E s_n(z)}+o(1)$, where, again, $o(1)$ will depend on $z$ and ``inverting'' the Stieltjes transform.
\end{enumerate}
\begin{remark}
	Note that instead of step 4 one could have plugged in the semicircle distribution and simplified the proof by just checking that this is indeed the limit. This is not done here because we want to see how the Stieltjes transform method can be used to derive such a conclusion without knowing about it beforehand.
	
	Also, there are other proofs (e.g. \cite{scGOE}) specifically for the GOE/GUE (instead of the more general Wigner matrices) which exploit their symmetries to shorten the proof considerably.
\end{remark}

\subsection{Preliminary Reductions}\label{ssec:prelimreduct}
\begin{lemma}
	For the matrices $M_n$ as given in \ref{thm:semicircle} it can be assumed without loss of generality that the diagonal entries are zero and the absolute values $|[M_n]_{ij}|$ are bounded by some constant $C>0$ which does not depend on $i,j$ or $n$.
\end{lemma}
\begin{proof}
	For every $n$ define the normalized random variable $\overline X_n$ by setting the diagonal elements to zero and bounding every entry by zero\footnote{Since the distribution changes we also need to recenter them by subtracting its mean value.}, i.e.

	$$[\overline X_n]_{ij}:=\begin{cases}
						[M_n]_{ij}/\sqrt n\mathbf 1_{|[M_n]_{ij}|\leq C}-\mathbb E\left[[M_n]_{ij}\mathbf 1_{|[M_n]_{ij}|\leq C}\right], &\mbox{if } i\neq j \\
						0, &\mbox{if } i=j
					  \end{cases}.$$
	
	Now we want to show that convergence of $\mu_{\overline X_n}$ to $\mu$ implies convergence of $\mu_{X_n}$ to the same limit $\mu$. To do so it suffices to show that $|\int\varphi\diff\mu_{X_n}-\int\varphi\diff\mu_{\overline X_n}|\rightarrow 0$ for every smooth $\varphi$ with compact support. In particular every such $\varphi$ is a Lipschitz function with Lipschitz-constant $\|\varphi'\|_{L^\infty}$. By denoting the eigenvalues of $X_n,\overline X_n$ as $\lambda_i,\overline\lambda_i$ and invoking the Wielandt-Hoffmann inequality we get the following bound:
	
	$$|\int\varphi\diff\mu_{X_n}-\int\varphi\diff\mu_{\overline X_n}| \leq
	\|\varphi'\|_{L^\infty}\frac{1}{n}\sum_{i=1}^n|\lambda_i-\overline\lambda_i| \leq$$
	$$
	\|\varphi'\|_{L^\infty}\left[\frac{1}{n}\sum_{i=1}^n|\lambda_i-\overline\lambda_i|^2\right]^{1/2} \leq
	\|\varphi'\|_{L^\infty}\left[\frac{1}{n}tr\left((X_n-\overline X_n)^2\right)\right]^{1/2}.$$
	
	Now we need to show that for every $\varepsilon,\delta > 0$ we can find a $C$ such that $\mathbb P\left(\frac{1}{n}tr\left((X_n-\overline X_n)^2\right)>\varepsilon\right)<\delta$ for large enough $n$.
	
	Using the definitions one sees that
	
	$$\frac{1}{n}tr\left((X_n-\overline X_n)^2\right) \leq
	\frac{1}{n^2}\sum_{i\neq j}\sqrt n\left[X_n-\overline X_n\right]_{ij}^2+\frac{1}{n}\sum_i [X_n]_{ii}^2,$$
	
	where the second term vanishes almost surely as $n\rightarrow\infty$ according to the strong law of large numbers. So we can use this to bound the probability of the event $\frac{1}{n}tr\left((X_n-\overline X_n)^2\right)>\varepsilon$ by
	
	$$
	\mathbb P\left(\frac{1}{n}tr\left((X_n-\overline X_n)^2\right)\right)\leq
	\frac{1}{n^2}\sum_{i\neq j}\mathbb P\left(\sqrt n[X_n-\overline X_n]_{ij}^2>\varepsilon\right).
	$$
	
	To see that this goes to zero for large $C$ we apply Markov's inequality, yielding 
	
	\begin{align*}
		&\mathbb P(\sqrt n[X_n-\overline X_n]_{ij}^2>\varepsilon)\leq
	\frac{1}{\varepsilon}\mathbb E\left[[\sqrt n X_n-\overline X_n]_{ij}^2\right]\leq\\
	&\frac{\sqrt n}{\varepsilon}\left(\mathbb E[[X_n]_{ij}^2\mathbf 1_{\sqrt n |[X_n]_{ij}|>C}]+\mathbb E[[X_n]_{ij}\mathbf 1_{\sqrt n |[X_n]_{ij}|>C}]^2\right).
	\end{align*}
	
	Since the entries $\sqrt n [X_n]_{ij}=[M_n]_{ij}$ have finite variance the right hand side will go to zero as $C\rightarrow\infty$ which proves the claim.
\end{proof}

\begin{theorem}[Talagrand's concentration inequality]
	Let $K>0$ and $X_1,\dots,X_n$ be independent complex variables with $|X_i|<K$ for all $1\leq i\leq n$. Identifying $\mathbb C$ with $\mathbb R^2$, let $F:\mathbb C^n\rightarrow\mathbb R$ a $1$-Lipschitz, convex function. Then for every $\lambda>0$ one has
	$$\mathbb P(|F(X)-MF(X)|\geq \lambda K)\leq C\exp^{-c\lambda^2}$$ and $$\mathbb P(|F(X)-\mathbb EF(X)|\geq \lambda K)\leq C\exp^{-c\lambda^2}$$
	for some absolute constants $c,C>0$, where $MF(X)$ is the median of $F(X)$.
\end{theorem}
For the proof refer to \url{https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/#boof0}.

\begin{theorem}[Weilandt-Hoffmann inequality]
	For Hermitian\footnote{The original Weilandt-Hoffmann inequality holds for normal operators, but we will restrict ourselves to Hermitian ones for simplicity's sake here.} $A,B$, where $\|B\|_F^2:=tr(B^2)^{\frac{1}{2}}$ is the Frobenius norm, we have
	$$\sum_{j=1}^n|\lambda_j(A+B)-\lambda_j(A)|^2\leq\|B\|_F^2.$$
\end{theorem}

\begin{proof}
	We fix $B$ to be $B-A$ and show the equivalent statement $$\sum_{i=1}^n |\lambda_i(A)-\lambda_i(B)|^2\leq tr\left((A-B)^2\right)$$
	
	and denote by $D_M$ the diagonal matrix $\text{diag}(\lambda_1(M),\dots,\lambda_n(M))$ and $U$ the matrix that diagonalises $B=UD_BU^*$ in the basis of $A$, such that $tr(AB)=tr(D_A U D_B U^*)=\sum_{i,j}\lambda_i(A)\lambda_j(B)|[U]_{ij}|^2.$
	
	Now we have to get some bounds on the ``worst case'' scenario, that is when $tr(AB)$ is big. To do so we define $v_{ij}:=|[U]_{ij}|^2$ and notice that the above $tr(AB)$ is linear in $v_{ij}$. Due to the orthogonality of $U$ one has $\sum_i v_{ij}=\sum_j v_{ij}=1$ so we can bound $tr(AB)$ from above by $\sup_{v_{ij}\geq 0,\sum_i v_{ij}=\sum_j v_{ij}=1}\sum_{i,j}\lambda_i(A)\lambda_j(B)v_{ij}.$
	
	This is an optimization problem of a linear functional over a convex set of doubly stochastic matrices. Hence the maximum is attained at the extreme points which are exactly the permutations. Of all the permutations the identity gives the maximal value which, after writing out the first inequality proves the theorem.
\end{proof}

\begin{theorem}[Chernoff inequality]
	Let $X_1,\dots,X_n$ be independent scalar random variables with $|X_i|\leq K$ almost surely, mean $\mu_i$ and variance $\sigma_i^2$. Then for any $\lambda>0$ there are absolute constants $c,C>0$ such that 
	
	$$\mathbb P(|S_n-\lambda|\geq\lambda\sigma)\leq C\max(e^{-c\lambda^2}, e^{-c\lambda\sigma/K}),$$
	
	where $\lambda:=\sum_i \lambda_i$ and $\sigma^2:=\sum_i \sigma_i^2$.
\end{theorem}
\begin{proof}
	We begin with some preliminary reductions, namely that it is sufficient to assume $X_i$ to be real valued by taking real and imaginary parts. Furthermore without loss of generality we set $K=1$ by dividing $X_i$ through $K$, center $X_i$ (i.e. set $\mu_i$ to zero by subtracting its mean), and, by symmetry, only show the upper tail estimate $\mathbb P(S_n\geq\lambda\sigma)\leq C\max(e^{-c\lambda^2}, e^{-c\lambda\sigma})$ (with different constants $c,C$).
	
	To do so we compute the exponential moments $\mathbb E \exp(tS_n)$ for some $0\leq t\leq 1$. By independence of the $X_i$ we have $\mathbb E \exp(tS_n)=\prod_{i=1}^n \mathbb E \exp(tX_i)$. The $\exp(tX_i)$ can be expanded into a Taylor series $1+tX_i+O(t^2X_i^2\exp(O(t)))$ which, after taking expectation and noting that $|X_i|\leq 1$, yields
	$$\mathbb E\exp(tX_i)=1+O(t^2\sigma_i^2\exp(O(t)))=\exp(O(t^2\sigma_i^2))$$
	
	and hence $\mathbb E\exp(tS_n)=\exp(O(t^2\sigma^2))$. By Markov's inequality we conclude with $$\mathbb P(S_n\geq\lambda\sigma)\leq\exp(O(t^2\sigma^2)-t\lambda\sigma),$$ which proves the claim after minimising the right hand side in $t\in[0,1]$.
\end{proof}

\begin{theorem}[McDiarmid's inequality]
	Let $X_1,\dots,X_n$ be independent random variables taking values in ranges $R_1,\dots,R_n$ and let $F:R_1\times\dots\times R_n\rightarrow\mathbb C$, such that for every $1\leq i\leq n$ we have $|F(x_1,\dots,x_i,\dots,x_n)-F(x_1,\dots,x_i',\dots,x_n)|\leq c_i$. Then for any $\lambda>0$ one has $$\mathbb P(|F(X)-\mathbb EF(X)|>\lambda\sigma)\leq C\exp^{-c\lambda^2},$$ for some absolute\footnote{Constants that maintain the same value wherever they occur. In particular applying McDiarmid's inequality in different settings we do not need to consider $C_n, c_n$, but can still write $C,c$.} constants $c,C>0$ and $\sigma=\sum_{i=1}^n c_i^2$.
\end{theorem}

\begin{proof}
	Similar as above we may assume that $F$ is real, it suffices to show the upper tail estimate $\mathbb P(F(X)-\mathbb EF(X)\geq\lambda\sigma^2)\leq Ce^{-c\lambda^2}$ and try to bound the exponential moment $\mathbb Ee^{tF(X)}$.
	
	To get a better idea how the exponential moment behaves we write it in such a way that we can use the fact that $F$ does not fluctuate ``too much'', i.e. we consider the conditional expectation $\mathbb E(e^{tF(X)}|X_1,\dots,X_{n-1})$ for $X_1,\dots,X_{n-1}$ fixed and write this as
	
	$$\mathbb E(e^{tF(X)}|X_1,\dots,X_{n-1}) = \mathbb E(e^{tY}|X_1,\dots,X_{n-1})e^{t\mathbb E(F(X)|X_1,\dots,X_{n-1})},$$
	where $Y:=F(X)-\mathbb E(F(X)|X_1,\dots,X_{n-1})$.
	
	Now we want to control the first term on the right hand side.
	Since $tY$ has mean zero and variance $t^2c_n^2$ we can (just in the proof of Chernoff's inequality) expand to get a Taylor series and take expectations to get
	
	$$\mathbb E(e^{tY}|X_1,\dots,X_{n-1}) \leq e^{O(t^2c_n^2)}.$$
	
	Integrating out the conditioning (note that $X_i$ are independent) we get the following bound on the exponential moment
	
	$$\mathbb Ee^{tF(X)} \leq e^{O(t^2c_n^2)}\mathbb E e^{t\mathbb E(F(X)|X_1,\dots,X_{n-1})}.$$
	
	Basically we reduced the problem by one dimension and noting that $\mathbb E(F(X)|X_1,\dots,X_{n-1})$ is a function $F_{n-1}(X_1,\dots,X_{n-1}$ with the same properties as in the theorem's assumptions on $F$ the above calculations can be performed iteratively $n$ times to get the upper bound
	
	$$\mathbb Ee^{tF(X)}\leq \exp(\sum_{i=1}^n O(t^2c_i^2) + t\mathbb EF(X)).$$
	
	Dividing by $\exp(t\mathbb EF(X)))$ and applying Markov's inequality yields
	
	$$\mathbb P(F(X)-\mathbb EF(X)\geq\lambda\sigma) \leq \exp(O(t^2\sigma^2)-t\lambda\sigma),$$
	
	which, after minimising the right hand side in $0\leq t\leq 1$, proves the claim.
\end{proof}


\subsection{Stieltjes Transform}

\begin{definition}[Stieltjes transform]
	For a probability measure $\mu$ we write $s_\mu$ for its Stieltjes transform $$\int_\mathbb{R} \frac{1}{x-z}\diff\mu(x).$$
\end{definition}
As mentioned above, $s_n$ will be a shorthand for $s_{\mu_{M_n/\sqrt{n}}}$.

\begin{lemma}[Properties of the Stieltjes transform]\label{lm:stieltjesproperties}
In the following let $\mu$ be some probability measure.
	\begin{enumerate}
		\item For $z=a+ib$ we have $Im\frac{1}{x-z}=\frac{b}{(x-a)^2+b^2}>0$.\label{lm:stieltjesproperties1}
		\item $s_\mu$ is analytic in $\mathbb C\setminus supp(\mu)\supset\mathbb C_+$.\label{lm:stieltjesproperties2}
		\item We can bound the absolute value as well as the derivatives by $|\frac{d^j}{dz^j}s_\mu(z)|\leq O(|Im(z)|^{-(j+1)})$ for all $j\in\mathbb \{0,1,\dots\}$.\label{lm:stieltjesproperties3}
	\end{enumerate}
\end{lemma}

\begin{proof}
	The first property is trivial, the second one can be seen by integrating $s_\mu$ over any contour not containing the support of $\mu$, interchanging the order of integration and noting that integrating $\frac{1}{x-z}$ gives $0$ by Cauchy's integral formula ($\frac{1}{x-z}$ being holomorphic outside the support of $\mu$). The Stieltjes transform being holomorphic (and thus analytic) follows by Morera's theorem.
	
	The third property can be obtained by using $\frac{1}{x-z}\leq \frac{1}{Im(z)}$ and using Cauchy's integral formula integrating this inequality.
\end{proof}

\begin{corollary}
	From \ref{lm:stieltjesproperties}.\ref{lm:stieltjesproperties1} it follows that $s_\mu$ is a Herglotz function and thus (e.g. \cite{TeschlQM}) $Im(s_\mu(.+ib))\rightharpoonup\pi\mu$ as $b\rightarrow 0^+$ in the vague topology or equivalently (by $\overline{s_\mu(z)}=s_\mu(\overline z)$)
	\begin{equation}
		\frac{s_\mu(.+ib)-s_\mu(.-ib)}{2\pi i}\rightharpoonup\mu.
	\end{equation}
	Note that this can also be seen by writing $Im(s_\mu)$ as the convolution $\pi\mu * P_b(a)$ with the Poisson kernels $P_b(x):=\frac{1}{\pi}\frac{b}{x^2+b^2} = \frac{1}{b}P_1(\frac{x}{b})$ which form a family of approximations to the identity.
	%This ``intuition'' will become important for the next proof and when applying Cauchy's interlacing law to show that $s_n(z)=s_{n-1}(z)+O(\frac{1}{n})$.
\end{corollary}

\begin{theorem}[Stieltjes continuity theorem]
	For $\mu_n$ (realisations of) random measures and $\mu$ a deterministic measure the following statement holds:
	
	$\mu_n\rightharpoonup\mu$ almost surely in the vague topology if and only if $s_{\mu_n}(z)\rightarrow s_\mu(z)$ almost surely for every $z\in\mathbb C_+$.
\end{theorem}

\begin{proof}
%	$$\mathbb P(\{\limsup_{n\rightarrow\infty}d_v(\mu_n,\mu)=0\})=1$$
%	$$\mathbb P(\{\forall \phi\in C_c(\mathbb R): \lim_{n\rightarrow\infty}\int_\mathbb{R}\phi \diff\mu_n=\int_\mathbb{R}\phi \diff\mu\})=1$$
%	$$\forall z\in\mathbb C_+:\mathbb P(\{\lim_{n\rightarrow\infty} s_{\mu_n}(z)=s_\mu(z)\})=1$$

	``$\Rightarrow$'': If $\mu_n\rightharpoonup\mu$ in the vague topology almost surely against a deterministic limit $\mu$, then $\forall \phi\in C_c(\mathbb R): \lim_{n\rightarrow\infty}\int_\mathbb{R}\phi \diff\mu_n=\int_\mathbb{R}\phi \diff\mu$ by definition and, by taking the completion, for all bounded, continuous functions vanishing at infinity. The function $x\mapsto \frac{1}{x-z}$ for some $z\in\mathbb C$ with $Im(z)>0$ is bounded and continuous on $\mathbb R$ and hence $s_{\mu_n}(z)\rightarrow s_\mu(z)$ almost surely.
	
	``$\Leftarrow$'': One can, up to an arbitrary small error $\varepsilon>0$, approximate $\int_\mathbb{R}\phi\diff\mu$ by $\int_\mathbb{R}\phi*P_b \diff\mu = \frac{1}{\pi}\int_\mathbb{R}\phi(a)s_\mu(a+ib)\diff a$ (and analogously for $\mu_n$).
	Thus we have $\frac{1}{\pi}\int_\mathbb{R}\phi(a)(s_\mu(a+ib)-s_{\mu_n}(a+ib))\diff a$ being equal to the difference (we are interested in) $\int_\mathbb{R}\phi\diff\mu - \int_\mathbb{R}\phi\diff\mu_n$ up to an error $\varepsilon$ and by dominated convergence (the Stieltjes transform of a measure is bounded for every $z\in \mathbb C_+$ and vanishes outside some compact set) we have convergence in the vague topology.
\end{proof}

\subsection{Stableness and Concentration of Measure}
In the following we keep using the notation as defined in \ref{thm:semicircle}. To show that $s_n(z)=s_{n-1}(z)+O_z(1/n)$ we first need to prove the following theorem:

\begin{theorem}[Cauchy's interlacing theorem]
	For any $n\times n$ Hermitian matrix $A_n$ with top left minor $A_{n-1}$ and eigenvalues of descending order ($\lambda_i\geq\lambda_{i+1}$) we have:
	\begin{equation*}
		\lambda_{i+1}(A_n)\leq\lambda_i(A_{n-1})\leq\lambda_i(A_n), 
	\end{equation*}
	for all $1\leq i < n$.
\end{theorem}
\begin{proof}
Using the min-max/max-min theorems ($\lambda_i(A)=\inf_{dim(V)=n-i+1}\sup_{v\in V : \|v\|=1}\langle Av,v\rangle$ and $\lambda_i(A)=\sup_{dim(V)=i}\inf_{v\in V : \|v\|=1}\langle Av,v\rangle$ respectively, c.f. \cite{TeschlQM} p.141) and writing $S_{n-i+1}$ for $\{v\in span\{a_i,\dots,a_n\}: \|v\|=1\}$, where $A_{n-1}a_j=\lambda_j a_j$ and $P$ an orthogonal projection such that $P^*A_nP=A_{n-1}$ we have
	\begin{align*}
		\lambda_i(A_{n-1}) =
		&\sup_{v\in S_i,\|v\|=1}v^*A_{n-1}v =\\
		&\sup_{v\in S_i,\|v\|=1}v^*P^*A_nPv\geq\\
		&\inf_{dim(V)=n-i}\sup_{v\in V,\|v\|=1}v^*A_nv =
		\lambda_{i+1}(A_n),
	\end{align*}
	and
	\begin{align*}
		\lambda_i(A_n) =
		&\inf_{dim(V)=n-i+1}\sup_{v\in V,\|v\|=1}v^*A_nv \geq\\
		&\sup_{v\in S_i,\|v\|=1}v^*P^*A_nPv =\\
		&\sup_{v\in S_i,\|v\|=1}v^*A_{n-1}v =
		\lambda_i(A_{n-1}).%,
	\end{align*}
	%where $S_i:=span\{a_i,\dots,a_{n-1}\}$. Note that we implicitly used the isometric embedding $\mathbb C^{n-1}\rightarrow\mathbb C^n$ given by $(x_1,\dots,x_{n-1})\mapsto(x_1,\dots,x_{n-1},0)$ to make sense of the equation $\langle \underbrace{P^*A_nP}_{n-1\times n-1}v,v\rangle = \langle \underbrace{A_n}_{n\times n}Pv,Pv\rangle$.
\end{proof}

Remembering the identity $Im(s_{\mu_n(a+ib)})=\pi\mu*P_b(a)$ and that $supp\mu_n$ consists of finitely many points, we have $Im(s_{\mu_n})=\pi\frac{1}{n}\sum_{\lambda_i}\frac{b}{(\lambda_i-a)^2+b^2}$ which suggests that it is important to take a closer look at the function $x\mapsto\frac{b}{(x-a)^2+b^2}$ to compare $s_{\mu_n}$ with $s_{\mu_{n-1}}$.

\begin{lemma}
	For fixed $z\in\mathbb C_+$ the Stieltjes transform is ``stable'' in $n$, i.e.
	\begin{equation*}
		s_n(z)=s_{n-1}(z)+O\left(\frac{1}{n}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	The idea is to use the Cauchy interlacing law and apply it to the previously mentioned identity by seeing that
	
	$$\sum_{j=1}^{n-1}\frac{b}{\lambda_j(M_{n-1})/\sqrt{n}-a}-\sum_{j=1}^n\frac{b}{\lambda_j(M_n)/\sqrt{n}-a}$$
	
	Up to the dimensional factors these two sums correspond to $s_n$ and $s_{n-1}$ and because of Cauchy's interlacing law this is an alternating sum, giving
	
	$$\sqrt{n(n-1)}s_{n-1}(\sqrt{n/(n-1)}(a+ib))-ns_n(a+ib)=O(1).$$
	
	Now using the fact that the Stieltjes transform $s_n$ is analytic away from the support of $\mu_n$ (\ref{lm:stieltjesproperties}.\ref{lm:stieltjesproperties2}) and using the bound for its derivatives (\ref{lm:stieltjesproperties}.\ref{lm:stieltjesproperties3}) we can approximate $s_{n-1}(\cdot)$ by $s_{n-1}(\sqrt{n/(n-1) \;\cdot\; )}$ and hence the statement holds.
\end{proof}

Using McDiarmid's inequality one gets 

\begin{equation}\label{eq:concentrationOfStieltjesTransform}
	\mathbb P(|s_n(z)-\mathbb Es_n(z)|\geq\lambda/\sqrt n)\leq C\exp^{-c\lambda^2},
\end{equation}
 for all $\lambda>0$ and some constants $c,C>0$.

From the Borel-Cantelli lemma we see that for every $z$ away from the real line $s_n(z)-\mathbb Es_n(z)$ converges almost surely to zero since, for every fixed $\varepsilon>0$, the sum $\sum_n \mathbb P(d(s_n-\mathbb Es_n,0)\geq\varepsilon) \leq C\sum_n\exp^{-cn\varepsilon^2}<\infty$ which is obtained by setting $\lambda=\varepsilon\sqrt n$.

\subsection{Finding the Semicircle Law}

We start off by using the following identity $$s_n(z) := \int_\mathbb{R}\frac{1}{x-z}d\mu_n(x) = \frac{1}{n}tr\left(\frac{1}{\sqrt n}M_n-zI_n\right)^{-1},$$ which holds for every $z\in\mathbb C\setminus supp(\mu_n)$. Because of the linearity of the trace we also have

\begin{equation}\label{eq:schursComplementForEs}
	\mathbb Es_n(z) = \frac{1}{n}\sum_{j=1}^n\mathbb E\left[\left(\frac{1}{\sqrt n}M_n-zI_n\right)^{-1}\right]_{jj}=\mathbb E\left[\left(\frac{1}{\sqrt n}M_n-zI_n\right)^{-1}\right]_{nn},
\end{equation}

where the last equality holds because all of the random variables $\left[(M_n/\sqrt n-zI_n)^{-1}\right]_{jj}$ have the same distribution.

To calculate one entry of an inverse of a matrix we use Schur's complement, which tells us that (under the assumptions that all the occurring inverse matrices exist) $$[(M_n/\sqrt n-zI_n)^{-1}]_{nn}=-\left(z+\frac{1}{n}X^*(\frac{1}{\sqrt n}M_{n-1}-zI_{n-1})^{-1}X\right)^{-1},$$ where $X\in\mathbb C^{n-1}$ is the top right column of $M_n$ with the bottom entry removed and the diagonal elements have been set to zero as justified in \ref{ssec:prelimreduct}.

This inverse exists because, for $z\in\mathbb C_+$, the imaginary part $Q:=Im\left((\frac{1}{\sqrt n}M_{n-1}-zI_{n-1})^{-1}\right)$ is positive definite according to the spectral theorem. To see this notice that this holds for arbitrary Hermitian matrices $M$ (instead of $\frac{1}{\sqrt n}M_{n-1}$) since their spectrum is on the real line. Thus, by the spectral theorem, we can write $Q=Im\int\frac{1}{\mu_M-z}dM(\mu_M)$ for some projection valued measure $dM$ and since $x\mapsto\frac{1}{x-z}$ is a Herglotz function its imaginary part will be greater than zero for $z\in\mathbb C_+$. As a result the imaginary part of the integrand (which is the imaginary part of the eigenvalues) will be greater than zero.
We conclude by noticing that $Im(z)>0$ plus something of the form $\langle Qx,x\rangle$ for $Q\geq 0$ will have imaginary part strictly greater than zero and hence the inverse exists.

The next step is to get a better understanding of the resolvent $R:=(\frac{1}{\sqrt n}M_{n-1}-zI_{n-1})^{-1}$ and its product $\langle RX,X\rangle$. Clearly $R$ and $X$ are independent, so we may treat $R$ almost like a deterministic matrix. Furthermore, again due to the spectral theorem, $\|R\|_{op}$ is at most $O(1)$. By the strong law of large numbers $\|X\|=O(\sqrt n)$ almost surely and by Chernoff's inequality this holds with overwhelming probability.

In the following we will show some results for some deterministic matrix $A$ which has roughly the same properties as $R$ (i.e. $A\geq 0$ and $\|A\|_{OP}=O(1)$).

Noting that the function $X\mapsto \sqrt{\langle AX,X\rangle}$ is a Lipschitz function with operator norm $O(1)$ and remembering from \ref{ssec:prelimreduct} that we can safely assume the entries to be bounded, we can invoke Talagrand's concentration inequality to get 

$$\mathbb P(|\sqrt{\langle AX,X\rangle}-\mathbb M\sqrt{\langle AX,X\rangle}|\geq\lambda)\leq C\exp^{-c\lambda^2}$$

for any $\lambda>0$. On the other hand we have $\sqrt{\langle AX,X\rangle}=O(\|X\|)=O(\sqrt n)$ with overwhelming probability (since the operator norm in the non-deterministic case is only controlled with overwhelming probability). Hence the median $\mathbb M\sqrt{\langle AX,X\rangle}=O(\sqrt n)$ and considering the square $\langle AX,X\rangle$, we conclude that

$$\mathbb P(|\langle AX,X\rangle-\mathbb M\langle AX,X\rangle|\geq\lambda\sqrt n)\leq C\exp^{-c\lambda^2}$$

with some (possibly different) $c,C>0$, since taking the square of $\sqrt{\langle AX,X\rangle}$ amounts to getting an additional factor $O(\sqrt n)$ and the median is of the same magnitude as the random variable.

Because of this concentration of measure result we may replace the median with the expected value, yielding

\begin{equation}\label{eq:concentrationOfXRXAroundTrR}
	\mathbb P(|\langle AX,X\rangle-\mathbb E\langle AX,X\rangle|\geq\lambda\sqrt n)\leq C\exp^{-c\lambda^2}
\end{equation}

for the the case where $A$ is deterministic and positive definite. One can extend this result to arbitrary matrices of operator norm $O(1)$ by noting that it holds for Hermitian matrices $M=M^*=M_+-M_-$ ($M_+\geq 0,M_-\geq 0,\|M\|_{OP}=O(1)$) by applying the triangle inequality and for general matrices $M$ (which are diagonalizable almost surely) of operator norm $\|M\|_{op}=O(1)$ by using their diagonalization $M=U^*DU=U^*D_+U-U^*D_-U$, $U$ unitary and $D_+,D_-\geq 0$, to get the bound $|X^*MX|\leq|X^*U^*D_+UX|+|X^*U^*D_-UX|$.

\begin{remark}\label{rem:conditionalExpectationForDeterministicResult}
	By using conditional expectations the above results also hold true for random matrices $R$ with $R\geq 0$ and $\|R\|_{OP}=O(1)$ as long as it is independent of $X$. The idea is to write all the above statements as $\mathbb P(E|\{A=R\})=\frac{\mathbb P(E\cap\{A=R\})}{\mathbb P(\{A=R\})}=\mathbb P(E)$ for all events $E$.
\end{remark}

Now we want to know what $\mathbb E\langle RX,X\rangle$ actually is. Because of the linearity of the expectation we write it as $\sum_{i,j=1}^{n-1}\mathbb E[\overline{X_i}R_{ij}X_j]$. Since the $X_i$ and $R_{ij}$ are independent we can write that as $\sum_{i,j=1}^{n-1}\mathbb E[\overline{X_i}X_j]\mathbb E[R_{ij}]$, but as the $X_i$ are iid with mean zero and variance one this double sum simplifies to the expectation of the trace of $R$ $$\sum_{i=1}^{n-1}\mathbb ER_{ii}.$$

Noticing that, up to some ``almost correct'' normalization factors, $tr(R)=tr\left((M_{n-1}/\sqrt n-zI_{n-1})^{-1}\right)$ is the Stieltjes transform $s_{n-1}(z)$. To be more precise we have $$tr(R)=n\sqrt{\frac{n}{n-1}}s_{n-1}\left(\sqrt{\frac{n}{n-1}}z\right),$$
but because of the smoothness of the Stieltjes transform for $z\in\mathbb C_+$ these factors do not play a role in the limit $n\rightarrow\infty$, i.e. $tr(R)=n(s_{n-1}(z)+o(1))$.

So using the concentration of measure results for the Stieltjes transform (\ref{eq:concentrationOfStieltjesTransform}) and for $\langle AX,X\rangle$(\ref{eq:concentrationOfXRXAroundTrR}), remembering that latter also holds for random matrices as long as they are independent (\ref{rem:conditionalExpectationForDeterministicResult}), we see that

$$\langle RX,X\rangle=n(s_{n-1}(z)+o(1))$$

with overwhelming probability. Substituting back in Schur's complement (\ref{eq:schursComplementForEs}) we get\footnote{Note that we need the concentration of measure results from above to justify ``interchanging'' expectation and taking the resolvent!} $$\mathbb Es_n(z)=-(z+\mathbb Es_n(z))^{-1}+o(1).$$

To say something about the limit we first need to ensure $\lim_{n\rightarrow\infty}\mathbb Es_n$ exists. This is indeed the case since $\mathbb Es_n$ is locally equicontinuous and locally uniformly bounded away from the real line. Applying the Arzel\'{a}-Ascoli theorem we get the existence of a subsequence that converges locally uniformly to a limit $s$, which is again a Herglotz function. Note that, by the concentration of measure for Stieltjes transforms, there is only one possible limit (so $\lim_{n\rightarrow\infty}\mathbb Es_n$ is well defined) and $s_n(z)$ even converges almost surely to $s(z)$. As a further result we get

$$s(z)=-(z+s(z))^{-1},$$

where the quadratic formula gives $$s(z)=-\frac{z\pm\sqrt{z^2-4}}{2}$$

From $\lim_{a\rightarrow\infty}s_\mu(a+ib)=0$ for every Stieltjes transform of a fixed measure $\mu$ we see that we need to take $s(z)=\frac{-z+\sqrt{z^2-4}}{2}$.

We conclude the proof with the Stieltjes inversion formula, yielding the famous result

$$\frac{s(.+ib)-s(.-ib)}{2\pi i}\rightharpoonup\frac{1}{2\pi}\sqrt{4-x^2}_+\diff x=\mu_{sc}$$

as $b\rightarrow 0^+$, which can be verified by an application of the Cauchy integral formula.
















