To be able to prove the above results we first need to prove some other theorems which require some 

\section{Gaussian Random Fields}
% motivation
% definitions
% results for the isotropic/stationary case
\cite{Adler07} p.121 for \begin{equation}
	\mathbb E\left\{\frac{\partial^k f(s)}{\partial s_{i_1}\dots\partial s_{i_k}} \frac{\partial^k f(t)}{\partial t_{i_1}\dots\partial t_{i_k}}\right\}=\frac{\partial^{2k} C(s,t)}{\partial s_{i_1}t_{i_1}\dots\partial s_{i_k}t_{i_k}}
\end{equation}

\section{GOE}
\begin{definition}
	Symmetric $N\times N$ matrices $H=H_N$ with $\mathbb E H_{ij}=0$ and $\mathbb E H_{ij}=1+\delta_{ij}$.
\end{definition}
\begin{remark}[Density in the space of matrices]
	Their density is given by the Gaussian measure
	\begin{equation}
		d\mathbb P(H)=Z_N^{-1}\exp\left(-N\frac{1}{4}tr H^2\right)
	\end{equation}
	with normalization constant $Z_N=\int d\mathbb P(H)\prod_{1\leq i\leq j\leq N} dH_{ij}$ which is a shorter way of writing $Z_N^{-1}\exp\left\{-N\frac{1}{4}\left(2\sum_{i<j}^N H_{ij}^2+\sum_{i=j}^N H_{ij}^2\right)\right\}$.
\end{remark}

\begin{theorem}[Joint probability density of eigenvalues]
	The joint probability density $Q_N$ of the unordered eigenvalues $\{\lambda_i\}_{i=1}^N$ of the GOE is given by
	$$Q_N(\lambda_1,\dots,\lambda_N) = C_N\prod_{i<j}|\lambda_i-\lambda_j|\exp\left(-N\frac{1}{4}\sum_i \lambda_i^2\right),$$
	
	where, without loss of generality, we assumed the variances of the entries to be normalized to one and for some $C_N>0$ which is uniquely determined by normalisation.
	
	\todo{be careful, Cerny paper uses the convention with N in the exp function!}
\end{theorem}
\begin{proof}
	This proof will follow the one given in \url{http://web.math.princeton.edu/mathlab/projects/ranmatrices/yl/randmtx.PDF}.
	
	Let $H$ be some element of the GOE. Since it is symmetric there is a decomposition in $H=UDU^T$ with $U$ orthogonal and $D=\text{diag}(\lambda_1,\dots,\lambda_N)$. Thus we can write 
	\begin{equation}\label{eq:H_ijLinearityInEigenvalues}
		H_{ij} = \sum_k \lambda_i U_{ik}U_{jk}
	\end{equation}	
	and, using the orthogonality of $U$: $\sum_k U_{ki}U_{kj}=\delta_{ij}$. Using those results one can infer that $\sum_{i,j}H_{ij}^2=\sum_i \lambda_i^2$.
	
	The key idea is to make use of the change of variables formula to go from $\mathbb P(\lambda_1,\dots,\lambda_N,\alpha_1,\dots,\alpha_{N(N-1)/2})$ to $\mathbb P(H)$. One should think of the $\{\alpha_i\}_{i=1}^{N(N-1)/2}$ as the parameters that determine the matrix $U$, which, together with the eigenvalues, uniquely determines $H$.
	
	To do that we first need some information on the determinant of the Jacobian $J$ of the change of variables:
	From equation \ref{eq:H_ijLinearityInEigenvalues} we see the linearity of $H_{ij}$ in the eigenvalues $\lambda_k$ which implies that every entry $\partial H_{ij}/\partial\alpha_k$ is linear in the eigenvalues. Hence, $|J|$ has to be a polynomial of degree $N(N-1)/2$ in each of the eigenvalues. If two eigenvalues coincide $U$ cannot be uniquely determined anymore and thus the inverse of the transformation is not unique, meaning that $|J|=0$. So the determinant of the Jacobian must vanish for all $\lambda_i=\lambda_j, i\neq j$, which is achieved if it contains a factor $\lambda_i-\lambda_j$. However, there are exactly $N(N-1)/2$ such factors and since that is just the degree of the polynomial it follows that we have completely accounted for $J$'s dependence on the eigenvalues by writing $$|J|=\prod_{i<j}(\lambda_i-\lambda_j) h(\alpha_1,\dots,\alpha_{N(N-1)/2}).$$
	
	Now we can write
	$$\mathbb P(\lambda_1,\dots,\lambda_N,\alpha_1,\dots,\alpha_{N(N-1)/2}) = $$
	$$\mathbb P(H)|J|=Z_N^{-1}\exp\left(-N\frac{1}{4}\sum_i\lambda_i^2\right)|\prod_{i<j}(\lambda_i-\lambda_j) h(\alpha_1,\dots,\alpha_{N(N-1)/2})|.$$
	
	Integrating out the dependence on the $\alpha_k$ yields the desired result.
\end{proof}

\section{LDP}
\begin{definition}[Large deviation principle]\label{def:LDP}
	Given some separable completely metrizable topological space\footnote{Henceforth such a space will be referred to as Polish space.} $X$, a sequence of Borel probability measures $\{\mathbb P_n\}$ on $X$ is said to satisfy a \textbf{large deviation principle} with speed $\{a_n\}$ and rate $I:X\rightarrow [0,\infty]$ if $a_n$ goes to $+\infty$ and $I$ is some lower semi-continuous functional such that for each Borel measurable set $E\subseteq X$ we have
	
	$$\limsup_n a_n^{-1}\log(\mathbb P_n(E))\leq -\inf_{x\in \overline E}I(x)$$
	and
	$$\liminf_n a_n^{-1}\log(\mathbb P_n(E))\geq -\inf_{x\in E^\circ}I(x).$$
	
	The lower semi-continuity implies that the sets $\{x\in X: I(x)\leq c\}$ are closed in $X$ for all $c\geq 0$. If they are also compact for all $c\geq 0$, $I$ is called a \textbf{good rate function}.
\end{definition}

We are particularly interested in getting LDPs of the $k$-th largest eigenvalues of the GOE, but it turns out in order to get those we first need a LDP for the law of the empirical measure $\mu^N=\frac{1}{N}\sum_{i=1}^N\delta_{\lambda_i}$.

\begin{theorem}[LDP of Wigner's semicircle law for the GOE]
	Let $I(\mu):=\frac{1}{2}\left(\int x^2\diff\mu(x) - \Sigma(\mu) - \frac{3}{4}-\frac{1}{2}\log 2 \right)$, for $\Sigma(\mu):=\int\int\log|x-y|\diff\mu(x)\diff\mu(y)$. Then:
	\begin{enumerate}
		\item $I$ is well defined on the space of probability measures $\mathcal M(\mathbb{R})$ on $\mathbb{R}$ endowed with the weak topology and takes values in $[0,\infty]$.
		\item $I(\mu)=\infty \Leftrightarrow \int x^2\diff\mu(x)=\infty$ or $\exists A\subseteq\mathbb R: \mu(A)>0 \land \exp\{-\inf_{\nu\in\mathcal M(A)}\int\int \log|x-y|^{-1}\diff\mu(x)\diff\mu(y)\}=0$, where the latter condition is usually referred to as the existence of a set of ``null logarithmic capacity''.
		\item $I$ is a good rate function.
		\item $I$ is a convex function.
		\item $I$ achieves its unique minimum value at Wigner's semicircle law.
	\end{enumerate}
	In particular the law of the empirical measure $\mu^N$ satisfies a LDP with good rate function $I$ and speed $N^2$.
\end{theorem}
\begin{proof}
	We will only give a very rough outline of the proof that can be found in \cite{ArousLDPforWSL}. \todo{should I skip this section?}
	
	%To prove that $I$ is we	ll defined one splits it up in $I(\mu)=H(\mu)+C$. 
\end{proof}

With this we can state the main result of this section. Note that without loss of generality we set the variances of the entries of the GOE to $1$.
\begin{theorem}[LDP for the $k$-th largest eigenvalue of the GOE]
	For each fixed $k\geq 1$, the $k$-th largest eigenvalue $\lambda_{N-k+1}$ of the GOE satisfies a LDP with speed $N$ and good rate function
	$$I_k(x)=kI_1(x)=\begin{cases}
						k\int_2^x\sqrt{\frac{z^2}{4}-1}\diff z, &\mbox{if } x\geq 2 \\
						\infty, &\mbox{else.}
					  \end{cases}$$
\end{theorem}
\begin{proof}
	\todo{prove it}
\end{proof}

\section{Morse Theory}
% definitions
% morse functions dense in C2

\section{Spin Glass Models}
% definitions
% applications: physics, neural networks, etc.

\section{Intermediary Results}
To prove our main results we first need some refinements for the expected values of critical values.

\todo{define E\_GOE, rho and B Borel sets}
\begin{theorem}[Refinement of ${\mathbb E[Crt_{N,k}(B)]}$]
	For all $N$, $p\geq 2$ and $k\in\{0,\dots,N-1\}$ we have
	\todo{Kac-Rice formulae, AT07}
	\begin{equation}\label{thm:2.1}
		\mathbb E[Crt_{N,k}(B)]=2\sqrt{\frac{2}{p}}(p-1)^{\frac{N}{2}}\mathbb E_{GOE}^N\left[e^{-N\frac{p-2}{2p}(\lambda_k^N)^2}\bm 1\left\{\lambda_k^N\in\sqrt{\frac{p}{2(p-1)}}B \right\}\right]
	\end{equation} and
	\todo{sum up previous eq.}
	\begin{equation}\label{thm:2.2}
		\mathbb E[Crt_N(B)]=2N\sqrt{\frac{2}{p}}(p-1)^{\frac{N}{2}}\int_{\sqrt{\frac{p}{2(p-1)}}B}exp\left\{-\frac{N(p-2)x^2}{2p}\right\}\rho_N(x)dx
	\end{equation}
\end{theorem}

\begin{theorem}[LDP for $k$-th largest eigenvalue]
	The $k$-th largest eigenvalue $\lambda_{N-k+1}$ of the GOE of dimension $N$ with variance $\sigma^2 N^{-1}(1+\delta_{ij})$ \todo{define GOE} satisfies an LDP with speed $N$ and a good rate function\todo{define good rate function} 
	\begin{equation}\label{thm:A.1}
		I_k(x;\sigma)=k I_1(x;\sigma)=\begin{cases}
						k\int_{2\sigma}^x \sigma^{-1}\sqrt{(\frac{z}{2\sigma})^2-1}\diff z, &\mbox{if } x\geq 2\sigma \\
						\infty, &\mbox{otherwise}
					  \end{cases}.
	\end{equation}
\end{theorem}








